One question that kept resurfacing when I was working on my blog post on the LLaMA series of models was: “Is LLaMA truly open-source?” While “Open Source” sounds fairly straight-forward in concept, the “open source” standards when it comes to AI models are still emerging and different from the traditional software/algorithms. In fact, many recent open-source LLMs have custom licenses different from traditional open source definition.

While the landscape of AI models continues to evolve, the notion of "Open Source" in AI remains a topic of debate. This post is not about what the landscape is and what it should be. In this short post, let's look at the usage restrictions for LLaMA 3.1 and BLOOM, both of which have a custom licensing. Additionally, I attempt to list popular Open Source/Weights Large Language Models (LLMs) along with their respective licenses.

## Responsible AI Licenses (RAIL)

Responsible AI Licenses or RAIL is a category of license that was designed to ensure that the open sourced models are used and updated within an acceptable responsible AI framework. This is to ensure that the AI models are not used in harmful applications. However, these licenses are often not as permissive as it is stipulated in the open source definition. 

“Meta is committed to promoting safe and fair use of its tools and features, including Meta Llama 3. If you access or use Meta Llama 3, you agree to this Acceptable Use Policy (“Policy”).

- Meta Llama 3 Acceptable Use Policy

A large number of models and dataset on HuggingFace is under a RAIL-like license, including LLaMA. Other than the RAIL-like Acceptable Use Policy, other components required for reproducibility of the work are not public, including the dataset used in the model pre-training and fine-tuning, are reasons why many argue that LLaMA models are is “open-weights and not truly open-source.”

### LLaMA 3.1 Community License Agreement v/s BigScience RAIL 

LLaMA 3.1 Community License is a RAIL-like license and is very similar to the BigScience RAIL, which is used by BLOOM. Let's look into the specifics of both of these licenses.

Usage and Modifications. Both licenses are similar in their terms of usage and modifications. Licensees have the freedom to use, adapt and modify the models for their use-cases, with some restrictions for large entities in case of LLaMA 3.1. Both the licenses allow the creation of derivative works and don't charge royalties. The BigScience license explicitly states that it is perpetual and irrevocable but the LLaMA 3.1 license does not, and it is unclear if the rights granted will expire in the future.

Redistribution. Both licenses allow redistribution of the models or their derivatives with/without any modifications but each distribution must include a copy of the original license agreement. Additionally, LLaMA 3.1 mandates that “Built with LLaMA” be prominently displayed on any related documentation accompanying the redistribution.

Ownership and Derivative Works. Both licenses allow derivative works and the licensees are granted the ownership of any derivative works that they create from the using the model and materials. Additionally, both licenses provide the derivative works "AS IS" without warranties. They also limit the liability of the original creators and place the responsibility on the licensees to ensure the appropriate and lawful use of the models and their derivatives.

Commercial Use. The LLaMA 3.1 license has an explicit, user-based restriction i.e., if a service exceeds more than 700 million monthly users in the preceding calendar month, then the licensee should request a commercial license from Meta, which Meta reserves the right to grant or deny this license at its sole discretion. The BigScience RAIL has no such explicit restriction.

Use Restrictions. Both licenses outline use restrictions, which emphases on the safe and responsible use of the models, their materials and derivatives. The restrictions  prohibit uses that violate laws, harm minors, disseminate false information, defame others, and any criminal activity.

Overall, as can be seen from the above comparison between BigScience RAIL and LLaMA 3.1 Community License, all RAIL-like licenses have similar terms that allow redistribution, modifications and innovation, but with a framework that promotes the responsible use of the AI models. Some popular LLMs with RAIL-like custom licenses are listed in the table below.

| Model                                                                                  | Release Notes/Research Papers                                                                                                                                                                                                                                                                  | License                                                                                                                                    | Released by  | Additional Notes                                                                                                                                                                                                                                                                                                                                                                        |
|----------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------|--------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| [LLaMA 3.1](https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f) | "[Release Notes](https://ai.meta.com/blog/meta-llama-3-1/) <br> [Paper](https://arxiv.org/pdf/2407.21783/)"                                                                                                                                                                                      | [LLAMA 3.1 COMMUNITY LICENSE AGREEMENT](https://llama.meta.com/llama3_1/license/)                                                          | Meta AI      |                                                                                                                                                                                                                                                                                                                                                                                            |
| [OPT 350M](https://huggingface.co/facebook/opt-350m)                                     | "[Release Notes](https://ai.meta.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/) <br> [Paper](https://arxiv.org/pdf/2205.01068)"                                                                                                                                    | [OPT-175B LICENSE AGREEMENT](https://github.com/facebookresearch/metaseq/blob/main/projects/OPT/MODEL_LICENSE.md)                           | Meta AI      | Data Card                                                                                                                                                                                                                                                                                                                                                                                 |
| [BLOOM](https://huggingface.co/bigscience/bloom)                                         | "[Release Notes](https://bigscience.huggingface.co/blog/bloom) <br> [Paper](https://arxiv.org/pdf/2211.05100)"                                                                                                                                                                                  | [BigScience RAIL](https://huggingface.co/spaces/bigscience/license)                                                                        | BigScience   |                                                                                                                                                                                                                                                                                                                                                                                            |
| [Qwen2](https://huggingface.co/collections/Qwen/qwen2-6659360b33528ced941e557f)          | "[Release Notes](https://qwenlm.github.io/blog/qwen2/) <br> [Paper](https://arxiv.org/pdf/2407.10671)"                                                                                                                                                                                          | [Tongyi Qianwen LICENSE AGREEMENT](https://huggingface.co/Qwen/Qwen2-72B/blob/main/LICENSE)                                                 | Alibaba      |                                                                                                                                                                                                                                                                                                                                                                                            |
| [Nemotron 4 340B](https://huggingface.co/collections/nvidia/nemotron-4-340b-666b7ebaf1b3867caf2f1911) | "[Release Notes](https://research.nvidia.com/publication/2024-06_nemotron-4-340b) <br> [Paper](https://arxiv.org/pdf/2406.11704)"                                                                                                                                                               | [NVIDIA Open Model License Agreement](https://developer.download.nvidia.com/licenses/nvidia-open-model-license-agreement-june-2024.pdf)     | NVIDIA       |                                                                                                                                                                                                                                                                                                                                                                                            |
| [Mistral-Large-Instruct-2407](https://huggingface.co/mistralai/Mistral-Large-Instruct-2407) | [Release Notes](https://mistral.ai/news/mistral-large-2407/)                                                                                                                                                                                                                                   | [Mistral AI Research License](https://mistral.ai/licenses/MRL-0.1.md)                                                                      | Mistral AI   | For commercial use contact Mistral AI for Mistral Commercial License                                                                                                                                                                                                                                                                                                                       |
| [Codestral-22B-v0.1](https://huggingface.co/mistralai/Codestral-22B-v0.1)                | [Release Notes](https://mistral.ai/news/codestral/)                                                                                                                                                                                                                                             | [Mistral AI non-production license (MNPL)](https://mistral.ai/news/mistral-ai-non-production-license-mnpl/)                                 | Mistral AI   |                                                                                                                                                                                                                                                                                                                                                                                            |
| [DeepSeek-Prover-V1.5](https://huggingface.co/papers/2408.08152)                         | [Paper](https://www.arxiv.org/pdf/2408.08152)                                                                                                                                                                                                                                                   | [DEEPSEEK LICENSE AGREEMENT](https://huggingface.co/deepseek-ai/DeepSeek-Prover-V1.5-RL/blob/main/LICENSE-MODEL)                            | DeepSeek     | Other DeepSeek Models: <br> [DeepSeek-V2](https://huggingface.co/collections/deepseek-ai/deepseek-v2-669a1c8b8f2dbc203fbd7746) <br> [DeepSeekCoder-V2](https://huggingface.co/collections/deepseek-ai/deepseekcoder-v2-666bf4b274a5f556827ceeca) <br> [DeepSeek-Math](https://huggingface.co/collections/deepseek-ai/deepseek-math-65f2962739da11599e441681) <br> [DeepSeek-LLM](https://huggingface.co/collections/deepseek-ai/deepseek-llm-65f2964ad8a0a29fe39b71d8) <br> [DeepSeek-MoE](https://huggingface.co/collections/deepseek-ai/deepseek-moe-65f29679f5cf26fe063686bf) <br> [DeepSeek-VL](https://huggingface.co/collections/deepseek-ai/deepseek-vl-65f295948133d9cf92b706d3) |
| [Stable LM](https://huggingface.co/collections/stabilityai/stable-lm-650852cfd55dd4e15cdcb30a) | "[Release Notes](https://stability.ai/news/introducing-stable-lm-2-12b) <br> [Paper](https://arxiv.org/pdf/2402.17834)"                                                                                                                                                                         | [Stability AI Community License](https://huggingface.co/stabilityai/stablelm-2-12b/blob/main/LICENSE.md)                                   | Stability AI |                                                                                                                                                                                                                                                                                                                                                                                            |
| [Stable Code](https://huggingface.co/collections/stabilityai/stable-code-64f9dfb4ebc8a1be0a3f7650) | "[Release Notes](https://stability.ai/news/stable-code-2024-llm-code-completion-release) <br> [Paper](https://arxiv.org/pdf/2404.01226)"                                                                                                                                                        | [Stability AI Community License](https://huggingface.co/stabilityai/stablelm-2-12b/blob/main/LICENSE.md)                                   | Stability AI |                                                                                                                                                                                                                                                                                                                                                                                            |
| [Jamba](https://huggingface.co/ai21labs/Jamba-v0.1)                                       | [Release Notes](https://www.ai21.com/jamba)                                                                                                                                                                                                                                                     | [Jamba Open Model License](https://www.ai21.com/licenses/jamba-open-model-license)                                                         | ai21labs     |                                                                                                                                                                                                                                                                                                                                                                                            |
Additionally, listed below are some LLMs that have more permissive licensing than the ones listed above.

| Model                                                                                  | Release Notes/Research Papers                                                                                                                                                                                            | License   | Released by  | Additional Notes                                                                                                   |
|----------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------|--------------|--------------------------------------------------------------------------------------------------------------------|
| [OLMo Suite](https://huggingface.co/collections/allenai/olmo-suite-65aeaae8fe5b6b2122b46778) | "[Release Notes](https://allenai.org/olmo/release-notes) <br> [Paper](https://arxiv.org/pdf/2402.00838)"                                                                                                                     | Apache 2.0 | Allen AI     | Training data is public: [Dolma Dataset](https://huggingface.co/datasets/allenai/dolma)                                                                 |
| [Mistral-7B-v0.3](https://huggingface.co/mistralai/Mistral-7B-v0.3)                    | "[Release Notes](https://mistral.ai/news/announcing-mistral-7b/) <br> [Paper](https://arxiv.org/pdf/2310.06825)"                                                                                                             | Apache 2.0 | Mistral AI   |                                                                                                                    |
| [Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)                | "[Release Notes](https://mistral.ai/news/mixtral-of-experts/) <br> [Paper](https://arxiv.org/pdf/2401.04088)"                                                                                                               | Apache 2.0 | Mistral AI   |                                                                                                                    |
| [Mixtral-8x22B-v0.1](https://huggingface.co/mistralai/Mixtral-8x22B-v0.1)              | [Release Notes](https://mistral.ai/news/mixtral-8x22b/)                                                                                                                                                                     | Apache 2.0 | Mistral AI   |                                                                                                                    |
| [Mathstral-7B-v0.1](https://huggingface.co/mistralai/Mathstral-7B-v0.1)                | [Release Notes](https://mistral.ai/news/mathstral/)                                                                                                                                                                          | Apache 2.0 | Mistral AI   |                                                                                                                    |
| [Mamba-Codestral-7B-v0.1](https://huggingface.co/mistralai/Mamba-Codestral-7B-v0.1)    | [Release Notes](https://mistral.ai/news/codestral-mamba/)                                                                                                                                                                   | Apache 2.0 | Mistral AI   |                                                                                                                    |
| [Gemma 2](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315) | "[Release Notes](https://blog.google/technology/developers/google-gemma-2/) <br> [Paper](https://arxiv.org/pdf/2403.08295)"                                                                                                  | Apache 2.0 | Google       |                                                                                                                    |
| [T5](https://huggingface.co/google-t5/t5-base)                                         | [Paper](https://arxiv.org/pdf/1910.10683)                                                                                                                                                                                   | Apache 2.0 | Google       | Training details with dataset listed: [T5 Training Details](https://huggingface.co/google-t5/t5-base#training-details)|
| [RoBERTa](https://huggingface.co/FacebookAI/roberta-base)                              | [Paper](https://arxiv.org/abs/1907.11692)                                                                                                                                                                                   | MIT       | Meta/Facebook AI | Training dataset and procedure given: [RoBERTa Training Data](https://huggingface.co/FacebookAI/roberta-base)       |
| [Phi-3](https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3)   | "[Release Notes](https://azure.microsoft.com/en-us/blog/introducing-phi-3-redefining-whats-possible-with-slms/) <br> [Paper](https://arxiv.org/pdf/2404.14219)"                                                              | MIT       | Microsoft    |                                                                                                                    |
| [grok-1](https://huggingface.co/xai-org/grok-1)                                        | [Release Notes](https://x.ai/blog/grok-os)                                                                                                                                                                                  | Apache 2.0 | xAI          |                                                                                                                    |
| [MPT](https://huggingface.co/collections/mosaicml/mpt-6564f3d9e5aac326bfa22def)        | "[Release Notes (MPT-7B)](https://www.databricks.com/blog/mpt-7b) <br> [Release Notes (MPT-30B)](https://www.databricks.com/blog/mpt-30b)"                                                                                   | Apache 2.0 | Databricks   |                                                                                                                    |


Feel free to contribute if you would like to see other LLMs added to the list.


